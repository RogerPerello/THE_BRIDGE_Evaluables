{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ck4PufGcoNb2"
   },
   "source": [
    "# PEC 3 - Web Scraping Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KC-F1owwXIEv"
   },
   "source": [
    "En esta PEC vamos a **continuar trabajando el web scraping**. Vamos a prestar especial atenci√≥n al web scraping en streaming que es el objetivo del reto. Adem√°s, continuaremos explorando otras librer√≠as que nos permiten hacer web scraping, como request-html y SerPapi.\n",
    "\n",
    "Por tanto, la PEC se va a dividir en **3 PARTES**: Web Scraping en Streaming, Web Scraping con Requests-html y, Web Scraping con SerPapi.\n",
    "\n",
    "Mencionar que, en algunos ejercicios se va a motivar el uso de los selectores CSS y los XPath. \n",
    "**Los selectores CSS y XPath** son expresiones que permiten seleccionar elementos de un documento HTML basados en sus clases o en la ubicaci√≥n dentro del contenido. \n",
    "Una referencia interesante de los mismos la pod√©is encontrar en las siguientes dos p√°ginas web: https://www.w3schools.com/xml/xpath_syntax.asp, https://www.w3schools.com/cssref/css_selectors.asp "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXE7YpuEXzMf"
   },
   "source": [
    "_Ejemplo:_\n",
    "\n",
    "\n",
    "\n",
    "*   _p.intro.rellevant_: seleccionar√≠a los elementos _p√°rrafo_ con valores de classe iguales a 'intro' y 'rellevant'.  \n",
    "*   _div > p_ : selecciona todos los elementos _\\<p>_ donde el padre sea un elemento \\<div>.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-wRrPf7uoeG7"
   },
   "source": [
    "## Parte 2. Web Scraping con Requests-html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ps6A-eVUvLf9"
   },
   "source": [
    "La librer√≠a **request- Html**  es como una combinaci√≥n de la librer√≠a requests y BeautifulSoup. \n",
    "\n",
    "El punto m√°s fuerte es que tiene soporte completo para JavaScript, lo que significa que puede ejecutar JavaScript y nos permite, por tanto, hacer scraping de contenido generado din√°micamente.  Por ejemplo, una aplicaci√≥n muy com√∫n es acceder el contenido que est√° disponible en las p√°ginas siguientes a la primera, y que cuando navegamos accedemos a √©l presionando el bot√≥n de la p√°gina correspondiente.  \n",
    "\n",
    "Adem√°s, para ejecutar JavaScript, podemos usar tambi√©n el m√©todo render de la librer√≠a. \n",
    "\n",
    "Otra particularizaci√≥n de esta librer√≠a es que es necesario iniciar sesi√≥n antes de empezar con el scraping del contenido HTML y cerrar sesi√≥n cuando se termine. Es decir:\n",
    " \n",
    "\n",
    "\n",
    "```\n",
    "request_html importar HTMLSession\n",
    "session = HTMLSession\n",
    "r = session.get (url_base)\n",
    "r.html.render\n",
    "‚Ä¶.\n",
    "session.close()\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LaCWQacEsMfm"
   },
   "source": [
    "Adem√°s, esta librer√≠a permite seleccionar los elementos de los documentos html mediant selectores CSS y/o selectores XPath. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9AI42u9rmvA"
   },
   "source": [
    "La documentaci√≥n de esta librer√≠a, la cual es recomendable que revis√©is para trabajar esta parte, la pod√©is encontrar en el siguiente enlace:  \n",
    "\n",
    "https://requests.readthedocs.io/projects/requests-html/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MygGSU_quC3"
   },
   "source": [
    "Antes de empezar a trabajar con esta librer√≠a, es necesario instalarla puesto que Google Collab no la tiene instalada por defecto como ocurria con las librerias que hemos utilizado anteriormente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XDtQuPn3qx0M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests-html\n",
      "  Downloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
      "Collecting parse\n",
      "  Downloading parse-1.19.0.tar.gz (30 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\mrusso\\miniconda3\\envs\\the_bridge_22\\lib\\site-packages (from requests-html) (2.28.1)\n",
      "Collecting w3lib\n",
      "  Downloading w3lib-2.1.1-py3-none-any.whl (21 kB)\n",
      "Collecting pyquery\n",
      "  Downloading pyquery-1.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting fake-useragent\n",
      "  Downloading fake_useragent-1.1.1-py3-none-any.whl (50 kB)\n",
      "Collecting bs4\n",
      "  Using cached bs4-0.0.1-py3-none-any.whl\n",
      "Collecting pyppeteer>=0.0.14\n",
      "  Downloading pyppeteer-1.0.2-py3-none-any.whl (83 kB)\n",
      "Requirement already satisfied: certifi>=2021 in c:\\users\\mrusso\\miniconda3\\envs\\the_bridge_22\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (2021.10.8)\n",
      "Collecting websockets<11.0,>=10.0\n",
      "  Downloading websockets-10.4-cp39-cp39-win_amd64.whl (101 kB)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in c:\\users\\mrusso\\miniconda3\\envs\\the_bridge_22\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (1.26.12)\n",
      "Collecting pyee<9.0.0,>=8.1.0\n",
      "  Downloading pyee-8.2.2-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tqdm<5.0.0,>=4.42.1\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "Collecting importlib-metadata>=1.4\n",
      "  Downloading importlib_metadata-5.1.0-py3-none-any.whl (21 kB)\n",
      "Collecting appdirs<2.0.0,>=1.4.3\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.11.0-py3-none-any.whl (6.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\mrusso\\miniconda3\\envs\\the_bridge_22\\lib\\site-packages (from tqdm<5.0.0,>=4.42.1->pyppeteer>=0.0.14->requests-html) (0.4.4)\n",
      "Collecting beautifulsoup4\n",
      "  Using cached beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\n",
      "Collecting soupsieve>1.2\n",
      "  Using cached soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\n",
      "Collecting importlib-resources>=5.0\n",
      "  Downloading importlib_resources-5.10.1-py3-none-any.whl (34 kB)\n",
      "Collecting cssselect>0.7.9\n",
      "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting lxml>=2.1\n",
      "  Downloading lxml-4.9.1-cp39-cp39-win_amd64.whl (3.6 MB)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\mrusso\\miniconda3\\envs\\the_bridge_22\\lib\\site-packages (from requests->requests-html) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mrusso\\miniconda3\\envs\\the_bridge_22\\lib\\site-packages (from requests->requests-html) (3.4)\n",
      "Building wheels for collected packages: parse\n",
      "  Building wheel for parse (setup.py): started\n",
      "  Building wheel for parse (setup.py): finished with status 'done'\n",
      "  Created wheel for parse: filename=parse-1.19.0-py3-none-any.whl size=24591 sha256=4cd677c4c98ee71ba68631e136f9cac894c29760bc0210916172146fecd14aeb\n",
      "  Stored in directory: c:\\users\\mrusso\\appdata\\local\\pip\\cache\\wheels\\d6\\9c\\58\\ee3ba36897e890f3ad81e9b730791a153fce20caa4a8a474df\n",
      "Successfully built parse\n",
      "Installing collected packages: zipp, soupsieve, websockets, tqdm, pyee, lxml, importlib-resources, importlib-metadata, cssselect, beautifulsoup4, appdirs, w3lib, pyquery, pyppeteer, parse, fake-useragent, bs4, requests-html\n",
      "Successfully installed appdirs-1.4.4 beautifulsoup4-4.11.1 bs4-0.0.1 cssselect-1.2.0 fake-useragent-1.1.1 importlib-metadata-5.1.0 importlib-resources-5.10.1 lxml-4.9.1 parse-1.19.0 pyee-8.2.2 pyppeteer-1.0.2 pyquery-1.4.3 requests-html-0.10.0 soupsieve-2.3.2.post1 tqdm-4.64.1 w3lib-2.1.1 websockets-10.4 zipp-3.11.0\n"
     ]
    }
   ],
   "source": [
    "# Instalar libreria \n",
    "!pip install requests-html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNm94Bb7tT8f"
   },
   "source": [
    "Una vez instalada, vamos a proceder con el primer ejemplo ilustrativo que nos servir√° como guia para realizar el ejercicio pr√°ctico planteado a continuaci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cn_P5GQz2pTg"
   },
   "source": [
    "En este ejemplo, vamos a hacer scraping al contenido de la conocida p√°gina de noticias _Reddit_ (https://reddit.com). De ella, vamos a extraer los titulares y cu√°ntas votaciones tiene cada noticia. Adem√°s, vamos a hacer la selecci√≥n de este contenido mediante _selectores CSS_ para empezar a familiarizarnos con ellos.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4wbaQLN3b12"
   },
   "source": [
    "Si presetamos atenci√≥n a la p√°gina, podemos ver que hay contenido que se carga de forma din√°mica y/o mediante botones. Particularmente, el contenido se va cargando cuando hacemos \"scrolldown\", mientras que tambien podemos cargar el contenido asociado a una cuenta, mediante el boton \"Sign up\". Por tanto, utilizar la librer√≠a **requests_html** podr√≠a ser recomendable si se quisiera capturar el contenido html que se carga con alguna de estas opciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_b-AfHR6H6w"
   },
   "source": [
    "En primer lugar, vamos a cargar la librer√≠a e iniciar sesi√≥n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QzTXRo6Mthgx"
   },
   "outputs": [],
   "source": [
    "# Cargar librer√≠a\n",
    "from requests_html import HTMLSession\n",
    "\n",
    "# Iniciar sesi√≥n\n",
    "session = HTMLSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGcC1WnP6h2M"
   },
   "source": [
    "Ahora, hacemos la solicitud y comprobamos cuantos html tenemos en la respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "ubj5Yr1516Qv",
    "outputId": "a9b484c8-7dcc-43f8-9f4c-87e691278e1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HTML url='https://www.reddit.com/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m r \u001b[39m=\u001b[39m session\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mhttps://reddit.com\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfor\u001b[39;00m html \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39mhtml:\n\u001b[0;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(html)\n",
      "File \u001b[1;32mc:\\Users\\Roger\\miniconda3\\envs\\data_analytics\\lib\\site-packages\\requests_html.py:481\u001b[0m, in \u001b[0;36mHTML.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    479\u001b[0m \u001b[39myield\u001b[39;00m \u001b[39mnext\u001b[39m\n\u001b[0;32m    480\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 481\u001b[0m     \u001b[39mnext\u001b[39m \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m\u001b[39m.\u001b[39;49mnext(fetch\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, next_symbol\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext_symbol)\u001b[39m.\u001b[39mhtml\n\u001b[0;32m    482\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Roger\\miniconda3\\envs\\data_analytics\\lib\\site-packages\\requests_html.py:463\u001b[0m, in \u001b[0;36mHTML.next\u001b[1;34m(self, fetch, next_symbol)\u001b[0m\n\u001b[0;32m    460\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m:\n\u001b[0;32m    461\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 463\u001b[0m __next \u001b[39m=\u001b[39m get_next()\n\u001b[0;32m    464\u001b[0m \u001b[39mif\u001b[39;00m __next:\n\u001b[0;32m    465\u001b[0m     url \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_absolute(__next)\n",
      "File \u001b[1;32mc:\\Users\\Roger\\miniconda3\\envs\\data_analytics\\lib\\site-packages\\requests_html.py:441\u001b[0m, in \u001b[0;36mHTML.next.<locals>.get_next\u001b[1;34m()\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_next\u001b[39m():\n\u001b[1;32m--> 441\u001b[0m     candidates \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfind(\u001b[39m'\u001b[39;49m\u001b[39ma\u001b[39;49m\u001b[39m'\u001b[39;49m, containing\u001b[39m=\u001b[39;49mnext_symbol)\n\u001b[0;32m    443\u001b[0m     \u001b[39mfor\u001b[39;00m candidate \u001b[39min\u001b[39;00m candidates:\n\u001b[0;32m    444\u001b[0m         \u001b[39mif\u001b[39;00m candidate\u001b[39m.\u001b[39mattrs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mhref\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    445\u001b[0m             \u001b[39m# Support 'next' rel (e.g. reddit).\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Roger\\miniconda3\\envs\\data_analytics\\lib\\site-packages\\requests_html.py:212\u001b[0m, in \u001b[0;36mBaseParser.find\u001b[1;34m(self, selector, containing, clean, first, _encoding)\u001b[0m\n\u001b[0;32m    207\u001b[0m     containing \u001b[39m=\u001b[39m [containing]\n\u001b[0;32m    209\u001b[0m encoding \u001b[39m=\u001b[39m _encoding \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoding\n\u001b[0;32m    210\u001b[0m elements \u001b[39m=\u001b[39m [\n\u001b[0;32m    211\u001b[0m     Element(element\u001b[39m=\u001b[39mfound, url\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39murl, default_encoding\u001b[39m=\u001b[39mencoding)\n\u001b[1;32m--> 212\u001b[0m     \u001b[39mfor\u001b[39;00m found \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpq(selector)\n\u001b[0;32m    213\u001b[0m ]\n\u001b[0;32m    215\u001b[0m \u001b[39mif\u001b[39;00m containing:\n\u001b[0;32m    216\u001b[0m     elements_copy \u001b[39m=\u001b[39m elements\u001b[39m.\u001b[39mcopy()\n",
      "File \u001b[1;32mc:\\Users\\Roger\\miniconda3\\envs\\data_analytics\\lib\\site-packages\\requests_html.py:149\u001b[0m, in \u001b[0;36mBaseParser.pq\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39m\"\"\"`PyQuery <https://pythonhosted.org/pyquery/>`_ representation\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[39mof the :class:`Element <Element>` or :class:`HTML <HTML>`.\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pq \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 149\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pq \u001b[39m=\u001b[39m PyQuery(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlxml)\n\u001b[0;32m    151\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pq\n",
      "File \u001b[1;32mc:\\Users\\Roger\\miniconda3\\envs\\data_analytics\\lib\\site-packages\\requests_html.py:160\u001b[0m, in \u001b[0;36mBaseParser.lxml\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lxml \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    159\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lxml \u001b[39m=\u001b[39m soup_parse(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhtml, features\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mhtml.parser\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m    161\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lxml \u001b[39m=\u001b[39m lxml\u001b[39m.\u001b[39mhtml\u001b[39m.\u001b[39mfromstring(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw_html)\n",
      "File \u001b[1;32mc:\\Users\\Roger\\miniconda3\\envs\\data_analytics\\lib\\site-packages\\lxml\\html\\soupparser.py:33\u001b[0m, in \u001b[0;36mfromstring\u001b[1;34m(data, beautifulsoup, makeelement, **bsargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfromstring\u001b[39m(data, beautifulsoup\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, makeelement\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbsargs):\n\u001b[0;32m     22\u001b[0m     \u001b[39m\"\"\"Parse a string of HTML data into an Element tree using the\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39m    BeautifulSoup parser.\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39m    used.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m     \u001b[39mreturn\u001b[39;00m _parse(data, beautifulsoup, makeelement, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbsargs)\n",
      "File \u001b[1;32mc:\\Users\\Roger\\miniconda3\\envs\\data_analytics\\lib\\site-packages\\lxml\\html\\soupparser.py:78\u001b[0m, in \u001b[0;36m_parse\u001b[1;34m(source, beautifulsoup, makeelement, **bsargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m bsargs:\n\u001b[0;32m     77\u001b[0m         bsargs[\u001b[39m'\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m  \u001b[39m# use Python html parser\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m tree \u001b[39m=\u001b[39m beautifulsoup(source, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbsargs)\n\u001b[0;32m     79\u001b[0m root \u001b[39m=\u001b[39m _convert_tree(tree, makeelement)\n\u001b[0;32m     80\u001b[0m \u001b[39m# from ET: wrap the document in a html root element, if necessary\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Roger\\miniconda3\\envs\\data_analytics\\lib\\site-packages\\bs4\\__init__.py:333\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39minitialize_soup(\u001b[39mself\u001b[39m)\n\u001b[0;32m    332\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 333\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_feed()\n\u001b[0;32m    334\u001b[0m     success \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    335\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Roger\\miniconda3\\envs\\data_analytics\\lib\\site-packages\\bs4\\__init__.py:451\u001b[0m, in \u001b[0;36mBeautifulSoup._feed\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[39m# Convert the document to Unicode.\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39mreset()\n\u001b[1;32m--> 451\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuilder\u001b[39m.\u001b[39;49mfeed(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmarkup)\n\u001b[0;32m    452\u001b[0m \u001b[39m# Close out any unfinished strings and close all the open tags.\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mendData()\n",
      "File \u001b[1;32mc:\\Users\\Roger\\miniconda3\\envs\\data_analytics\\lib\\site-packages\\bs4\\builder\\_htmlparser.py:399\u001b[0m, in \u001b[0;36mHTMLParserTreeBuilder.feed\u001b[1;34m(self, markup)\u001b[0m\n\u001b[0;32m    397\u001b[0m parser\u001b[39m.\u001b[39msoup \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoup\n\u001b[0;32m    398\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 399\u001b[0m     parser\u001b[39m.\u001b[39;49mfeed(markup)\n\u001b[0;32m    400\u001b[0m     parser\u001b[39m.\u001b[39mclose()\n\u001b[0;32m    401\u001b[0m \u001b[39mexcept\u001b[39;00m HTMLParseError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Roger\\miniconda3\\envs\\data_analytics\\lib\\html\\parser.py:110\u001b[0m, in \u001b[0;36mHTMLParser.feed\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Feed data to the parser.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[39mCall this as often as you want, with as little or as much text\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[39mas you want (may include '\\n').\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrawdata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrawdata \u001b[39m+\u001b[39m data\n\u001b[1;32m--> 110\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgoahead(\u001b[39m0\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Roger\\miniconda3\\envs\\data_analytics\\lib\\html\\parser.py:172\u001b[0m, in \u001b[0;36mHTMLParser.goahead\u001b[1;34m(self, end)\u001b[0m\n\u001b[0;32m    170\u001b[0m     k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_starttag(i)\n\u001b[0;32m    171\u001b[0m \u001b[39melif\u001b[39;00m startswith(\u001b[39m\"\u001b[39m\u001b[39m</\u001b[39m\u001b[39m\"\u001b[39m, i):\n\u001b[1;32m--> 172\u001b[0m     k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparse_endtag(i)\n\u001b[0;32m    173\u001b[0m \u001b[39melif\u001b[39;00m startswith(\u001b[39m\"\u001b[39m\u001b[39m<!--\u001b[39m\u001b[39m\"\u001b[39m, i):\n\u001b[0;32m    174\u001b[0m     k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_comment(i)\n",
      "File \u001b[1;32mc:\\Users\\Roger\\miniconda3\\envs\\data_analytics\\lib\\html\\parser.py:420\u001b[0m, in \u001b[0;36mHTMLParser.parse_endtag\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m    417\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_data(rawdata[i:gtpos])\n\u001b[0;32m    418\u001b[0m         \u001b[39mreturn\u001b[39;00m gtpos\n\u001b[1;32m--> 420\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle_endtag(elem)\n\u001b[0;32m    421\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclear_cdata_mode()\n\u001b[0;32m    422\u001b[0m \u001b[39mreturn\u001b[39;00m gtpos\n",
      "File \u001b[1;32mc:\\Users\\Roger\\miniconda3\\envs\\data_analytics\\lib\\site-packages\\bs4\\builder\\_htmlparser.py:193\u001b[0m, in \u001b[0;36mBeautifulSoupHTMLParser.handle_endtag\u001b[1;34m(self, name, check_already_closed)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malready_closed_empty_element\u001b[39m.\u001b[39mremove(name)\n\u001b[0;32m    192\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 193\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msoup\u001b[39m.\u001b[39;49mhandle_endtag(name)\n",
      "File \u001b[1;32mc:\\Users\\Roger\\miniconda3\\envs\\data_analytics\\lib\\site-packages\\bs4\\__init__.py:742\u001b[0m, in \u001b[0;36mBeautifulSoup.handle_endtag\u001b[1;34m(self, name, nsprefix)\u001b[0m\n\u001b[0;32m    736\u001b[0m \u001b[39m\"\"\"Called by the tree builder when an ending tag is encountered.\u001b[39;00m\n\u001b[0;32m    737\u001b[0m \n\u001b[0;32m    738\u001b[0m \u001b[39m:param name: Name of the tag.\u001b[39;00m\n\u001b[0;32m    739\u001b[0m \u001b[39m:param nsprefix: Namespace prefix for the tag.\u001b[39;00m\n\u001b[0;32m    740\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    741\u001b[0m \u001b[39m#print(\"End tag: \" + name)\u001b[39;00m\n\u001b[1;32m--> 742\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendData()\n\u001b[0;32m    743\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popToTag(name, nsprefix)\n",
      "File \u001b[1;32mc:\\Users\\Roger\\miniconda3\\envs\\data_analytics\\lib\\site-packages\\bs4\\__init__.py:588\u001b[0m, in \u001b[0;36mBeautifulSoup.endData\u001b[1;34m(self, containerClass)\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_only \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtagStack) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    584\u001b[0m        (\u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_only\u001b[39m.\u001b[39mtext \u001b[39mor\u001b[39;00m \\\n\u001b[0;32m    585\u001b[0m         \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_only\u001b[39m.\u001b[39msearch(current_data)):\n\u001b[0;32m    586\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> 588\u001b[0m containerClass \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstring_container(containerClass)\n\u001b[0;32m    589\u001b[0m o \u001b[39m=\u001b[39m containerClass(current_data)\n\u001b[0;32m    590\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobject_was_parsed(o)\n",
      "File \u001b[1;32mc:\\Users\\Roger\\miniconda3\\envs\\data_analytics\\lib\\site-packages\\bs4\\__init__.py:506\u001b[0m, in \u001b[0;36mBeautifulSoup.string_container\u001b[1;34m(self, base_class)\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[39m# On top of that, we may be inside a tag that needs a special\u001b[39;00m\n\u001b[0;32m    504\u001b[0m \u001b[39m# container class.\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstring_container_stack \u001b[39mand\u001b[39;00m container \u001b[39mis\u001b[39;00m NavigableString:\n\u001b[1;32m--> 506\u001b[0m     container \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuilder\u001b[39m.\u001b[39;49mstring_containers\u001b[39m.\u001b[39;49mget(\n\u001b[0;32m    507\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstring_container_stack[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mname, container\n\u001b[0;32m    508\u001b[0m     )\n\u001b[0;32m    509\u001b[0m \u001b[39mreturn\u001b[39;00m container\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "r = session.get('https://reddit.com')\n",
    "for html in r.html:\n",
    "    print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_oi_bku2ItU"
   },
   "source": [
    "Podemos observar que el html que devuelve es solo 1. Esto es porque el contenido se va actualizando y a√±adiendo din√°micamente mediante \"scrolldown\" y no mediante el click de un boton de Pagina siguiente, Pagina 2, o similares. En caso de haber un segundo html, este hace referencia al boton de introducir una cuenta reddit. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6kZWiN_p83uC"
   },
   "source": [
    "Ahora, despu√©s de inspeccionar la p√°gina, advertimos que el contenido relativo a los enlaces de las diferentes entradas o noticias de la p√°gina, est√° en la etiqueta 'a' y cuya classe es '_3ryJoIoycVkA88fy40qNJc'. Por tanto, usando selectores CSS, la instrucci√≥n que devolver√° el contenidos ser√°: *r.html.find('a._3ryJoIoycVkA88fy40qNJc')*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "YLYQts748d_-",
    "outputId": "38c57b0d-bccd-4a81-f051-29f65dbd4c47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 titulares aparecen en la primera p√°gina de reddit, el resto de titulares se corresponden con el contenido despu√©s de hacer scrolldowns\n"
     ]
    }
   ],
   "source": [
    "subreddit_1 = r.html.find('a._3ryJoIoycVkA88fy40qNJc')\n",
    "print(str(len(subreddit_1)) + ' titulares aparecen en la primera p√°gina de reddit, el resto de titulares se corresponden con el contenido despu√©s de hacer scrolldowns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "id": "jO9Nthx2-gTK",
    "outputId": "292c4693-3d31-421d-eebd-61d25ff3f96f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'https://www.reddit.com/r/spain/'},\n",
       " {'https://www.reddit.com/r/mildlyinfuriating/'},\n",
       " {'https://www.reddit.com/r/AskReddit/'},\n",
       " {'https://www.reddit.com/r/interestingasfuck/'},\n",
       " {'https://www.reddit.com/r/Genshin_Impact_Leaks/'},\n",
       " {'https://www.reddit.com/r/LMDShow/'},\n",
       " {'https://www.reddit.com/r/Genshin_Impact_Leaks/'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtener las url completas de cada entrada\n",
    "subreddit_url=[element.absolute_links for element in subreddit_1]\n",
    "subreddit_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bsoi9zZS-xev"
   },
   "source": [
    "Ahora vamos a obtener los t√≠tulos de las noticias. Este contenido se halla en la etiqueta 'h3' con classe '_eYtD2XCVieq6emjKBH3m'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "id": "ZGfKcOZ_-s4Y",
    "outputId": "999c2618-edf5-402a-8f41-246fee0f24ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ü§î',\n",
       " 'Um gajo pode sonhar n√£o pode?',\n",
       " 'Los Santos Drug Wars inyectar√° un nuevo caos psicoactivo en GTA Online el 13 de diciembre. √önete a una banda de inadaptados en el primer cap√≠tulo de una extensa nueva actualizaci√≥n de GTA Online, con una trama dividida en dos partes.',\n",
       " 'The fees on this Air BnB for one night.',\n",
       " 'What did you not know about sex until you lost your virginity?',\n",
       " \"U.S. bombs dropped on Laos. 270 million bombs were dropped on Laos in a span of 9 years, making it the most heavily bombed country in the history of the world. That's 57 bombs every minute on average.\",\n",
       " 'Alhaitham gameplay (R.I.P)',\n",
       " 'escucha escucha ‚òùÔ∏èü§ì',\n",
       " 'Yaoyao Gameplay']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Titulos\n",
    "subreddit_2 = r.html.find('h3._eYtD2XCVieq6emjKBH3m')\n",
    "subreddit_titulos=[element.text for element in subreddit_2]\n",
    "subreddit_titulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "pnyQxO56_8qf",
    "outputId": "9bcd54c8-6847-47b4-a0e8-b0912c2e9ff4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['357', '‚Ä¢', '17.0k', '10.9k', '20.8k', '4.7k', '412', '2.1k']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Votaciones\n",
    "subreddit_3 = r.html.find('div._1rZYMD_4xY3gRcSS3p8ODO._25IkBM0rRUqWX5ZojEMAFQ')\n",
    "subreddit_votaciones=[element.text for element in subreddit_3]\n",
    "subreddit_votaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7VLcfGBHcHh"
   },
   "source": [
    "Por √∫ltimo, cerraremos la sesi√≥n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "EPYBbWQiHmAa"
   },
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7H66P1kAjHT"
   },
   "source": [
    "Ahora vamos a construir el dataframe que contiene el resultado de los t√≠tulos y las voraciones. Atendiendo a los datos, el primer titular sin puntuaci√≥n lo vamos a excluir porque hace referencia a un anuncio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "4QGsFYZbBboW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_reddit=pd.DataFrame()\n",
    "df_reddit['t√≠tulo']=subreddit_titulos[1:]\n",
    "df_reddit['votaciones']=subreddit_votaciones[1:]\n",
    "df_reddit['url']=subreddit_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t√≠tulo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Um gajo pode sonhar n√£o pode?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Los Santos Drug Wars inyectar√° un nuevo caos psicoactivo en GTA Online el 13 de diciembre. √önete a una banda de inadaptados en el primer cap√≠tulo de una extensa nueva actualizaci√≥n de GTA Online, con una trama dividida en dos partes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The fees on this Air BnB for one night.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What did you not know about sex until you lost your virginity?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U.S. bombs dropped on Laos. 270 million bombs were dropped on Laos in a span of 9 years, making it the most heavily bombed country in the history of the world. That's 57 bombs every minute on average.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Alhaitham gameplay (R.I.P)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>escucha escucha ‚òùÔ∏èü§ì</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Yaoyao Gameplay</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                      t√≠tulo\n",
       "0  Um gajo pode sonhar n√£o pode?                                                                                                                                                                                                            \n",
       "1  Los Santos Drug Wars inyectar√° un nuevo caos psicoactivo en GTA Online el 13 de diciembre. √önete a una banda de inadaptados en el primer cap√≠tulo de una extensa nueva actualizaci√≥n de GTA Online, con una trama dividida en dos partes.\n",
       "2  The fees on this Air BnB for one night.                                                                                                                                                                                                  \n",
       "3  What did you not know about sex until you lost your virginity?                                                                                                                                                                           \n",
       "4  U.S. bombs dropped on Laos. 270 million bombs were dropped on Laos in a span of 9 years, making it the most heavily bombed country in the history of the world. That's 57 bombs every minute on average.                                 \n",
       "5  Alhaitham gameplay (R.I.P)                                                                                                                                                                                                               \n",
       "6  escucha escucha ‚òùÔ∏èü§ì                                                                                                                                                                                                                      \n",
       "7  Yaoyao Gameplay                                                                                                                                                                                                                          "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "DaWKTgJiB02L",
    "outputId": "9cd16d1f-f791-4c79-f022-553cf6f30333"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Roger\\AppData\\Local\\Temp\\ipykernel_2300\\2182009870.py:1: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t√≠tulo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Um gajo pode sonhar n√£o pode?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Los Santos Drug Wars inyectar√° un nuevo caos psicoactivo en GTA Online el 13 de diciembre. √önete a una banda de inadaptados en el primer cap√≠tulo de una extensa nueva actualizaci√≥n de GTA Online, con una trama dividida en dos partes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The fees on this Air BnB for one night.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What did you not know about sex until you lost your virginity?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U.S. bombs dropped on Laos. 270 million bombs were dropped on Laos in a span of 9 years, making it the most heavily bombed country in the history of the world. That's 57 bombs every minute on average.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Alhaitham gameplay (R.I.P)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>escucha escucha ‚òùÔ∏èü§ì</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Yaoyao Gameplay</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                      t√≠tulo\n",
       "0  Um gajo pode sonhar n√£o pode?                                                                                                                                                                                                            \n",
       "1  Los Santos Drug Wars inyectar√° un nuevo caos psicoactivo en GTA Online el 13 de diciembre. √önete a una banda de inadaptados en el primer cap√≠tulo de una extensa nueva actualizaci√≥n de GTA Online, con una trama dividida en dos partes.\n",
       "2  The fees on this Air BnB for one night.                                                                                                                                                                                                  \n",
       "3  What did you not know about sex until you lost your virginity?                                                                                                                                                                           \n",
       "4  U.S. bombs dropped on Laos. 270 million bombs were dropped on Laos in a span of 9 years, making it the most heavily bombed country in the history of the world. That's 57 bombs every minute on average.                                 \n",
       "5  Alhaitham gameplay (R.I.P)                                                                                                                                                                                                               \n",
       "6  escucha escucha ‚òùÔ∏èü§ì                                                                                                                                                                                                                      \n",
       "7  Yaoyao Gameplay                                                                                                                                                                                                                          "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "df_reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tD5CuNtYxtPb"
   },
   "source": [
    "En este ejemplo, hemos podido scrapear el contenido inicial que aparece en la p√°gina de Reddit. No obstante, el contenido que encontramos al hacer _scrolldown_ no lo hemos podido capturar.  \n",
    "\n",
    "Para poder capturar el resto de entradas, podemos hacer uso de la funcion *render()* de la librer√≠a *Request-html*. No obstante, para el uso de esta funci√≥n es necesario iniciar una sesi√≥n en modo asincrona. Los Jupyter notebooks presentan ciertos problemas para trabajar de esta forma y requieren la instalaci√≥n de _Chromium_; es por eso que en esta PEC no lo vamos a ver. A pesar de ello, a modo explicativo, se adjunta el c√≥digo que se requerir√≠a:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "import asyncio\n",
    "from requests_html import AsyncHTMLSession\n",
    "\n",
    "asession = AsyncHTMLSession()\n",
    "\n",
    "async def get_results():\n",
    "    r = await asession.get('https://reddit.com')\n",
    "    r.html.arender(scrolldown=10, sleep=1)\n",
    "    return r\n",
    "\n",
    "respuesta = asession.run(get_results)\n",
    "\n",
    "asession.close()\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p95Elq6CIWDQ"
   },
   "source": [
    "Otro ejemplo es la obtenci√≥n de informaci√≥n de una p√°gina que recoge los memes m√°s relevantes en la actualidad (www.knowyourmeme.com). En esta p√°gina, a diferencia del ejemplo anterior, el contenido est√° organizado por p√°ginas y se puede acceder a ellas a trav√©s de los t√≠picos botones de p√°agina 1, 2, 3,... Vamos a ver, por tanto, la respuesta que nos devolver√≠a requests-html en este caso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "3KRonTtQKoHk"
   },
   "outputs": [],
   "source": [
    "# Cargar librer√≠a\n",
    "from requests_html import HTMLSession\n",
    "\n",
    "# Iniciar sesi√≥n\n",
    "session = HTMLSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "UwVnuDSNLOX1",
    "outputId": "a16a1e0d-796f-4d8b-a3dd-48558e4c85aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 :  <HTML url='https://knowyourmeme.com/'>\n",
      "2 :  <HTML url='https://knowyourmeme.com/page/2'>\n",
      "3 :  <HTML url='https://knowyourmeme.com/page/3'>\n",
      "4 :  <HTML url='https://knowyourmeme.com/page/4'>\n",
      "5 :  <HTML url='https://knowyourmeme.com/page/5'>\n",
      "6 :  <HTML url='https://knowyourmeme.com/page/6'>\n",
      "7 :  <HTML url='https://knowyourmeme.com/page/7'>\n",
      "8 :  <HTML url='https://knowyourmeme.com/page/8'>\n",
      "9 :  <HTML url='https://knowyourmeme.com/page/9'>\n",
      "10 :  <HTML url='https://knowyourmeme.com/page/10'>\n"
     ]
    }
   ],
   "source": [
    "r2= session.get('https://knowyourmeme.com/')\n",
    "\n",
    "total_pags_scrap=10\n",
    "page=0\n",
    "for html in r2.html:\n",
    "    page+=1\n",
    "    print(page, ': ' ,html)\n",
    "    if page==total_pags_scrap:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "UxHydcKDMkKn"
   },
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1LJBx70NqQ4"
   },
   "source": [
    "En este caso, la respuesta contine los html asociados a las diferentes p√°ginas en las que est√° organizado el contenido. Como pod√©is observar, en el bucle se ha introducido un _break_. Esto es porque knowyourmeme.com ofrece la posibilidad de revisar los memes contenidos en  hasta m√°s de 9500 p√°ginas. Esperar a scrapear este total de p√°ginas nos llevar√≠a mucho m√°s tiempo y no es el objetivo de esta PEC.\n",
    "\n",
    "Considerando los ejemplos que se han facilitado, realizar el ejercicio pr√°ctico 3 para explorar la librer√≠a Requests-html y familiarizarse con los selectores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9r0z0MdxbSs4"
   },
   "source": [
    "### **Ejercicio pr√°ctico 2 (vuelos real time)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTucSQJybd9F"
   },
   "source": [
    "La API OpenSky (https://opensky-network.org/apidoc/rest.html) permite recuperar informaci√≥n del estado de los vuelos o aeronaves actualizados en tiempo real. Adem√°s, los usuarios con autorizaci√≥n pueden tener acceso a datos hist√≥ricos de la √∫ltima hora, indicando el intervalo de tiempo c√≥mo un par√°metro m√°s en la consulta. Para esta actividad vamos a utilizar la versi√≥n b√°sica de esta API y por tanto, no vamos a tener opci√≥n de seleccionar el intervalo de tiempo de inter√©s. No obstante, para la finalidad de la actividad, no va a hacer falta. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SwHLlsi7bjiW"
   },
   "source": [
    "En este ejercicio se solicita al estudiante que, despu√©s de la revisi√≥n de la documentaci√≥n de la API, obtenga el estado del aire (aviones y aeronaves) durante una secuencia de 10 consultas. Para ello, se recomienda que se implente un bucle donde en cada iteraci√≥n se solicite informaci√≥n a la API y se esperen 10 segundos entre una iteraci√≥n y la siguiente. La solicitud a la API se pide que se realice mediante la librer√≠a requests, con la que se trabaj√≥ en la PEC anterior. \n",
    "\n",
    "Mencionar que esta API permite a√±adir en la solicitud una acotaci√≥n del espacio a considerar. La acotaci√≥n del espacio a√©reo se corresponder√° con los siguientes valores de latitud i longitud:\n",
    "\n",
    "> lat_max= 72.822950; \n",
    "lon_min= -17.000158;\n",
    "lat_min= 35.550423;\n",
    "lon_max= 44.022436\n",
    "\n",
    "Para cada consulta, se solicita obtener el n√∫mero total de aeronaves agrupadas por pa√≠s de origen de la aeronave. Para ello, ser√° necesario revisar la documentaci√≥n de la API y explorar la forma de la respuesta para determinar cu√°l es el campo de inter√©s de los datos scrapeados para la realizaci√≥n del ejercicio. \n",
    "Adem√°s del n√∫mero de aeronaves por pa√≠s de origen, en cada iteraci√≥n tambi√©n hay que almacenar el valor temporal de la consulta (es decir, el campo 'time', que viene dado en segundos ).\n",
    "\n",
    "Con el resultado de las 10 iteraciones, se solicita que:\n",
    "- Se cree un dataframe (df_vuelos) que recoja el n√∫mero total de aeronaves por cada pa√≠s de origen (filas) para cada instante de tiempo (columnas). \n",
    "- Se a√±ada una columna 'mean_flights' y una 'percen_flights' que representen respectivamente el valor medio de vuelos de cada pa√≠s durante el intervalo de tiempo considerado con las 10 iteraciones y el porcentaje del pa√≠s respecto al total.\n",
    "- Representar con un diagrama de barras el n√∫mero de vuelos medios de los 15 pa√≠ses con mayor media de vuelos en el intervalo de tiempo considerado. El eje X, se corresponder√° con el pa√≠s y el eje Y con el valor de vuelos medios. Mostrar los valores ordenados de forma ascendiente. Mostrar tambi√©n el porcentaje agregado de estos 15 pa√≠ses respecto al total.\n",
    "- Repetir el putno anterior para los 15 pa√≠ses con menor media."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deZfeuJ6n67S"
   },
   "source": [
    "NOTAS PARA LA REALIZACI√ìN DE LAS SOLICITUDES Y LA CREACI√ìN DEL DATAFRAME: \n",
    "- Para considerar el tiempo de espera, como ya hemos hecho en casos anteriores, se recomienda el uso de la librer√≠a time y su funci√≥n time.sleep(n_segundos).\n",
    "- Debido a que estamos usando la versi√≥n gratuita de la API, el tiempo no se actualiza de forma constante y puede que haya solicitudes cuyo valor de 'time' es el mismo. En este caso, no almacenar el valor o eliminar despu√©s los valores cuyo 'time' est√© duplicado. Esto es para que cuando calculemos la media, no tenga efecto en la misma el valor duplicado.\n",
    "- _Sugerencia:_ En cada iteraci√≥n se puede crear un dataframe auxiliar con los resultados de la solicitud y cuyas columnas sean 'Country' y 'N' para el 'time' evaluado en dicha iteraci√≥n. Despu√©s, con este dataframe auxiliar, ir hacido join o merge al dataframe glogal que contendr√° los resultados de todas las iteraciones. Si se sigue esta sugerencia, utilizar la versi√≥n 'outer' de la funci√≥n merge de fomra que se consideren todos los pa√≠ses que han aparecido en todas las iteraciones aunque en una determinada iteraci√≥n no hubiese aeronaves de alguno de ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opensky_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "qEEfCaAvb69-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 :  <HTML url='https://opensky-network.org/apidoc/rest.html'>\n"
     ]
    }
   ],
   "source": [
    "#Cargar librerias\n",
    "import requests\n",
    "\n",
    "session = HTMLSession()\n",
    "\n",
    "r2= session.get('https://opensky-network.org/apidoc/rest.html')\n",
    "\n",
    "total_pags_scrap=10\n",
    "page=0\n",
    "for html in r2.html:\n",
    "    page+=1\n",
    "    print(page, ': ' ,html)\n",
    "    if page==total_pags_scrap:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir limites Europa\n",
    "#TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hs1Hq70Tb_DU"
   },
   "outputs": [],
   "source": [
    "# Creaci√≥n de df_vuelos mediante la realizaci√≥n de 10 solicitudes\n",
    "#TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostrar df_vuelos\n",
    "#TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear nuevas variables: 'mean_flights' y 'percen_flights'\n",
    "#TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafico de barras (15 pa√≠ses con m√°s vuelos) + calcular porcentaje de esos 15 primeros paises\n",
    "#TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yZgLoHcjcBWj"
   },
   "outputs": [],
   "source": [
    "# Grafico de barras (15 pa√≠ses con menos vuelos)\n",
    "#TODO\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ZYIvtVsuFTrD"
   ],
   "name": "CyPD_PEC3_2020_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "data_analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "b824bf52cf534a16238dce884e8fdba8cd1853727e048048d6b0b37b0d718f27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
