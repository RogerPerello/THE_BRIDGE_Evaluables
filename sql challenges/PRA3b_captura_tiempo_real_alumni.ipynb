{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ck4PufGcoNb2"
   },
   "source": [
    "# PEC 3 - Web Scraping Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KC-F1owwXIEv"
   },
   "source": [
    "En esta PEC vamos a **continuar trabajando el web scraping**. Vamos a prestar especial atención al web scraping en streaming que es el objetivo del reto. Además, continuaremos explorando otras librerías que nos permiten hacer web scraping, como request-html y SerPapi.\n",
    "\n",
    "Por tanto, la PEC se va a dividir en **3 PARTES**: Web Scraping en Streaming, Web Scraping con Requests-html y, Web Scraping con SerPapi.\n",
    "\n",
    "Mencionar que, en algunos ejercicios se va a motivar el uso de los selectores CSS y los XPath. \n",
    "**Los selectores CSS y XPath** son expresiones que permiten seleccionar elementos de un documento HTML basados en sus clases o en la ubicación dentro del contenido. \n",
    "Una referencia interesante de los mismos la podéis encontrar en las siguientes dos páginas web: https://www.w3schools.com/xml/xpath_syntax.asp, https://www.w3schools.com/cssref/css_selectors.asp "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXE7YpuEXzMf"
   },
   "source": [
    "_Ejemplo:_\n",
    "\n",
    "\n",
    "\n",
    "*   _p.intro.rellevant_: seleccionaría los elementos _párrafo_ con valores de classe iguales a 'intro' y 'rellevant'.  \n",
    "*   _div > p_ : selecciona todos los elementos _\\<p>_ donde el padre sea un elemento \\<div>.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-wRrPf7uoeG7"
   },
   "source": [
    "## Parte 2. Web Scraping con Requests-html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ps6A-eVUvLf9"
   },
   "source": [
    "La librería **request- Html**  es como una combinación de la librería requests y BeautifulSoup. \n",
    "\n",
    "El punto más fuerte es que tiene soporte completo para JavaScript, lo que significa que puede ejecutar JavaScript y nos permite, por tanto, hacer scraping de contenido generado dinámicamente.  Por ejemplo, una aplicación muy común es acceder el contenido que está disponible en las páginas siguientes a la primera, y que cuando navegamos accedemos a él presionando el botón de la página correspondiente.  \n",
    "\n",
    "Además, para ejecutar JavaScript, podemos usar también el método render de la librería. \n",
    "\n",
    "Otra particularización de esta librería es que es necesario iniciar sesión antes de empezar con el scraping del contenido HTML y cerrar sesión cuando se termine. Es decir:\n",
    " \n",
    "\n",
    "\n",
    "```\n",
    "request_html importar HTMLSession\n",
    "session = HTMLSession\n",
    "r = session.get (url_base)\n",
    "r.html.render\n",
    "….\n",
    "session.close()\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LaCWQacEsMfm"
   },
   "source": [
    "Además, esta librería permite seleccionar los elementos de los documentos html mediant selectores CSS y/o selectores XPath. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9AI42u9rmvA"
   },
   "source": [
    "La documentación de esta librería, la cual es recomendable que reviséis para trabajar esta parte, la podéis encontrar en el siguiente enlace:  \n",
    "\n",
    "https://requests.readthedocs.io/projects/requests-html/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MygGSU_quC3"
   },
   "source": [
    "Antes de empezar a trabajar con esta librería, es necesario instalarla puesto que Google Collab no la tiene instalada por defecto como ocurria con las librerias que hemos utilizado anteriormente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XDtQuPn3qx0M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests-html\n",
      "  Downloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
      "Collecting parse\n",
      "  Downloading parse-1.19.0.tar.gz (30 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\mrusso\\miniconda3\\envs\\the_bridge_22\\lib\\site-packages (from requests-html) (2.28.1)\n",
      "Collecting w3lib\n",
      "  Downloading w3lib-2.1.1-py3-none-any.whl (21 kB)\n",
      "Collecting pyquery\n",
      "  Downloading pyquery-1.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting fake-useragent\n",
      "  Downloading fake_useragent-1.1.1-py3-none-any.whl (50 kB)\n",
      "Collecting bs4\n",
      "  Using cached bs4-0.0.1-py3-none-any.whl\n",
      "Collecting pyppeteer>=0.0.14\n",
      "  Downloading pyppeteer-1.0.2-py3-none-any.whl (83 kB)\n",
      "Requirement already satisfied: certifi>=2021 in c:\\users\\mrusso\\miniconda3\\envs\\the_bridge_22\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (2021.10.8)\n",
      "Collecting websockets<11.0,>=10.0\n",
      "  Downloading websockets-10.4-cp39-cp39-win_amd64.whl (101 kB)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in c:\\users\\mrusso\\miniconda3\\envs\\the_bridge_22\\lib\\site-packages (from pyppeteer>=0.0.14->requests-html) (1.26.12)\n",
      "Collecting pyee<9.0.0,>=8.1.0\n",
      "  Downloading pyee-8.2.2-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tqdm<5.0.0,>=4.42.1\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "Collecting importlib-metadata>=1.4\n",
      "  Downloading importlib_metadata-5.1.0-py3-none-any.whl (21 kB)\n",
      "Collecting appdirs<2.0.0,>=1.4.3\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.11.0-py3-none-any.whl (6.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\mrusso\\miniconda3\\envs\\the_bridge_22\\lib\\site-packages (from tqdm<5.0.0,>=4.42.1->pyppeteer>=0.0.14->requests-html) (0.4.4)\n",
      "Collecting beautifulsoup4\n",
      "  Using cached beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\n",
      "Collecting soupsieve>1.2\n",
      "  Using cached soupsieve-2.3.2.post1-py3-none-any.whl (37 kB)\n",
      "Collecting importlib-resources>=5.0\n",
      "  Downloading importlib_resources-5.10.1-py3-none-any.whl (34 kB)\n",
      "Collecting cssselect>0.7.9\n",
      "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting lxml>=2.1\n",
      "  Downloading lxml-4.9.1-cp39-cp39-win_amd64.whl (3.6 MB)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\mrusso\\miniconda3\\envs\\the_bridge_22\\lib\\site-packages (from requests->requests-html) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mrusso\\miniconda3\\envs\\the_bridge_22\\lib\\site-packages (from requests->requests-html) (3.4)\n",
      "Building wheels for collected packages: parse\n",
      "  Building wheel for parse (setup.py): started\n",
      "  Building wheel for parse (setup.py): finished with status 'done'\n",
      "  Created wheel for parse: filename=parse-1.19.0-py3-none-any.whl size=24591 sha256=4cd677c4c98ee71ba68631e136f9cac894c29760bc0210916172146fecd14aeb\n",
      "  Stored in directory: c:\\users\\mrusso\\appdata\\local\\pip\\cache\\wheels\\d6\\9c\\58\\ee3ba36897e890f3ad81e9b730791a153fce20caa4a8a474df\n",
      "Successfully built parse\n",
      "Installing collected packages: zipp, soupsieve, websockets, tqdm, pyee, lxml, importlib-resources, importlib-metadata, cssselect, beautifulsoup4, appdirs, w3lib, pyquery, pyppeteer, parse, fake-useragent, bs4, requests-html\n",
      "Successfully installed appdirs-1.4.4 beautifulsoup4-4.11.1 bs4-0.0.1 cssselect-1.2.0 fake-useragent-1.1.1 importlib-metadata-5.1.0 importlib-resources-5.10.1 lxml-4.9.1 parse-1.19.0 pyee-8.2.2 pyppeteer-1.0.2 pyquery-1.4.3 requests-html-0.10.0 soupsieve-2.3.2.post1 tqdm-4.64.1 w3lib-2.1.1 websockets-10.4 zipp-3.11.0\n"
     ]
    }
   ],
   "source": [
    "# Instalar libreria \n",
    "!pip install requests-html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNm94Bb7tT8f"
   },
   "source": [
    "Una vez instalada, vamos a proceder con el primer ejemplo ilustrativo que nos servirá como guia para realizar el ejercicio práctico planteado a continuación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cn_P5GQz2pTg"
   },
   "source": [
    "En este ejemplo, vamos a hacer scraping al contenido de la conocida página de noticias _Reddit_ (https://reddit.com). De ella, vamos a extraer los titulares y cuántas votaciones tiene cada noticia. Además, vamos a hacer la selección de este contenido mediante _selectores CSS_ para empezar a familiarizarnos con ellos.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4wbaQLN3b12"
   },
   "source": [
    "Si presetamos atención a la página, podemos ver que hay contenido que se carga de forma dinámica y/o mediante botones. Particularmente, el contenido se va cargando cuando hacemos \"scrolldown\", mientras que tambien podemos cargar el contenido asociado a una cuenta, mediante el boton \"Sign up\". Por tanto, utilizar la librería **requests_html** podría ser recomendable si se quisiera capturar el contenido html que se carga con alguna de estas opciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_b-AfHR6H6w"
   },
   "source": [
    "En primer lugar, vamos a cargar la librería e iniciar sesión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QzTXRo6Mthgx"
   },
   "outputs": [],
   "source": [
    "# Cargar librería\n",
    "from requests_html import HTMLSession\n",
    "\n",
    "# Iniciar sesión\n",
    "session = HTMLSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGcC1WnP6h2M"
   },
   "source": [
    "Ahora, hacemos la solicitud y comprobamos cuantos html tenemos en la respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "ubj5Yr1516Qv",
    "outputId": "a9b484c8-7dcc-43f8-9f4c-87e691278e1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HTML url='https://www.reddit.com/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n",
      "<HTML url='https://www.reddit.com/topics/a-1/'>\n",
      "<HTML url='https://www.reddit.com/t/alberto_moreno/'>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m r \u001b[39m=\u001b[39m session\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mhttps://reddit.com\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfor\u001b[39;00m html \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39mhtml:\n\u001b[0;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(html)\n",
      "File \u001b[1;32mc:\\Users\\Roger\\miniconda3\\envs\\data_analytics\\lib\\site-packages\\requests_html.py:481\u001b[0m, in \u001b[0;36mHTML.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    479\u001b[0m \u001b[39myield\u001b[39;00m \u001b[39mnext\u001b[39m\n\u001b[0;32m    480\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 481\u001b[0m     \u001b[39mnext\u001b[39m \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m\u001b[39m.\u001b[39;49mnext(fetch\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, next_symbol\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext_symbol)\u001b[39m.\u001b[39mhtml\n\u001b[0;32m    482\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Roger\\miniconda3\\envs\\data_analytics\\lib\\site-packages\\requests_html.py:463\u001b[0m, in \u001b[0;36mHTML.next\u001b[1;34m(self, fetch, next_symbol)\u001b[0m\n\u001b[0;32m    460\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m:\n\u001b[0;32m    461\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 463\u001b[0m __next \u001b[39m=\u001b[39m get_next()\n\u001b[0;32m    464\u001b[0m \u001b[39mif\u001b[39;00m __next:\n\u001b[0;32m    465\u001b[0m     url \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_absolute(__next)\n",
      "File \u001b[1;32mc:\\Users\\Roger\\miniconda3\\envs\\data_analytics\\lib\\site-packages\\requests_html.py:441\u001b[0m, in \u001b[0;36mHTML.next.<locals>.get_next\u001b[1;34m()\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_next\u001b[39m():\n\u001b[1;32m--> 441\u001b[0m     candidates \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfind(\u001b[39m'\u001b[39;49m\u001b[39ma\u001b[39;49m\u001b[39m'\u001b[39;49m, containing\u001b[39m=\u001b[39;49mnext_symbol)\n\u001b[0;32m    443\u001b[0m     \u001b[39mfor\u001b[39;00m candidate \u001b[39min\u001b[39;00m candidates:\n\u001b[0;32m    444\u001b[0m         \u001b[39mif\u001b[39;00m candidate\u001b[39m.\u001b[39mattrs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mhref\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    445\u001b[0m             \u001b[39m# Support 'next' rel (e.g. reddit).\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Roger\\miniconda3\\envs\\data_analytics\\lib\\site-packages\\requests_html.py:212\u001b[0m, in \u001b[0;36mBaseParser.find\u001b[1;34m(self, selector, containing, clean, first, _encoding)\u001b[0m\n\u001b[0;32m    207\u001b[0m     containing \u001b[39m=\u001b[39m [containing]\n\u001b[0;32m    209\u001b[0m encoding \u001b[39m=\u001b[39m _encoding \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoding\n\u001b[0;32m    210\u001b[0m elements \u001b[39m=\u001b[39m [\n\u001b[0;32m    211\u001b[0m     Element(element\u001b[39m=\u001b[39mfound, url\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39murl, default_encoding\u001b[39m=\u001b[39mencoding)\n\u001b[1;32m--> 212\u001b[0m     \u001b[39mfor\u001b[39;00m found \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpq(selector)\n\u001b[0;32m    213\u001b[0m ]\n\u001b[0;32m    215\u001b[0m \u001b[39mif\u001b[39;00m containing:\n\u001b[0;32m    216\u001b[0m     elements_copy \u001b[39m=\u001b[39m elements\u001b[39m.\u001b[39mcopy()\n",
      "File \u001b[1;32mc:\\Users\\Roger\\miniconda3\\envs\\data_analytics\\lib\\site-packages\\requests_html.py:149\u001b[0m, in \u001b[0;36mBaseParser.pq\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39m\"\"\"`PyQuery <https://pythonhosted.org/pyquery/>`_ representation\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[39mof the :class:`Element <Element>` or :class:`HTML <HTML>`.\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pq \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 149\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pq \u001b[39m=\u001b[39m PyQuery(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlxml)\n\u001b[0;32m    151\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pq\n",
      "File \u001b[1;32mc:\\Users\\Roger\\miniconda3\\envs\\data_analytics\\lib\\site-packages\\requests_html.py:160\u001b[0m, in \u001b[0;36mBaseParser.lxml\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lxml \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    159\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lxml \u001b[39m=\u001b[39m soup_parse(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhtml, features\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mhtml.parser\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m    161\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lxml \u001b[39m=\u001b[39m lxml\u001b[39m.\u001b[39mhtml\u001b[39m.\u001b[39mfromstring(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw_html)\n",
      "File \u001b[1;32mc:\\Users\\Roger\\miniconda3\\envs\\data_analytics\\lib\\site-packages\\lxml\\html\\soupparser.py:33\u001b[0m, in \u001b[0;36mfromstring\u001b[1;34m(data, beautifulsoup, makeelement, **bsargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfromstring\u001b[39m(data, beautifulsoup\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, makeelement\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbsargs):\n\u001b[0;32m     22\u001b[0m     \u001b[39m\"\"\"Parse a string of HTML data into an Element tree using the\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39m    BeautifulSoup parser.\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39m    used.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m     \u001b[39mreturn\u001b[39;00m _parse(data, beautifulsoup, makeelement, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbsargs)\n",
      "File \u001b[1;32mc:\\Users\\Roger\\miniconda3\\envs\\data_analytics\\lib\\site-packages\\lxml\\html\\soupparser.py:78\u001b[0m, in \u001b[0;36m_parse\u001b[1;34m(source, beautifulsoup, makeelement, **bsargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m bsargs:\n\u001b[0;32m     77\u001b[0m         bsargs[\u001b[39m'\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m  \u001b[39m# use Python html parser\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m tree \u001b[39m=\u001b[39m beautifulsoup(source, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbsargs)\n\u001b[0;32m     79\u001b[0m root \u001b[39m=\u001b[39m _convert_tree(tree, makeelement)\n\u001b[0;32m     80\u001b[0m \u001b[39m# from ET: wrap the document in a html root element, if necessary\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Roger\\miniconda3\\envs\\data_analytics\\lib\\site-packages\\bs4\\__init__.py:333\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39minitialize_soup(\u001b[39mself\u001b[39m)\n\u001b[0;32m    332\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 333\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_feed()\n\u001b[0;32m    334\u001b[0m     success \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    335\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Roger\\miniconda3\\envs\\data_analytics\\lib\\site-packages\\bs4\\__init__.py:451\u001b[0m, in \u001b[0;36mBeautifulSoup._feed\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[39m# Convert the document to Unicode.\u001b[39;00m\n\u001b[0;32m    449\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39mreset()\n\u001b[1;32m--> 451\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuilder\u001b[39m.\u001b[39;49mfeed(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmarkup)\n\u001b[0;32m    452\u001b[0m \u001b[39m# Close out any unfinished strings and close all the open tags.\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mendData()\n",
      "File \u001b[1;32mc:\\Users\\Roger\\miniconda3\\envs\\data_analytics\\lib\\site-packages\\bs4\\builder\\_htmlparser.py:399\u001b[0m, in \u001b[0;36mHTMLParserTreeBuilder.feed\u001b[1;34m(self, markup)\u001b[0m\n\u001b[0;32m    397\u001b[0m parser\u001b[39m.\u001b[39msoup \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoup\n\u001b[0;32m    398\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 399\u001b[0m     parser\u001b[39m.\u001b[39;49mfeed(markup)\n\u001b[0;32m    400\u001b[0m     parser\u001b[39m.\u001b[39mclose()\n\u001b[0;32m    401\u001b[0m \u001b[39mexcept\u001b[39;00m HTMLParseError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Roger\\miniconda3\\envs\\data_analytics\\lib\\html\\parser.py:110\u001b[0m, in \u001b[0;36mHTMLParser.feed\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Feed data to the parser.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[39mCall this as often as you want, with as little or as much text\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[39mas you want (may include '\\n').\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrawdata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrawdata \u001b[39m+\u001b[39m data\n\u001b[1;32m--> 110\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgoahead(\u001b[39m0\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Roger\\miniconda3\\envs\\data_analytics\\lib\\html\\parser.py:172\u001b[0m, in \u001b[0;36mHTMLParser.goahead\u001b[1;34m(self, end)\u001b[0m\n\u001b[0;32m    170\u001b[0m     k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_starttag(i)\n\u001b[0;32m    171\u001b[0m \u001b[39melif\u001b[39;00m startswith(\u001b[39m\"\u001b[39m\u001b[39m</\u001b[39m\u001b[39m\"\u001b[39m, i):\n\u001b[1;32m--> 172\u001b[0m     k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparse_endtag(i)\n\u001b[0;32m    173\u001b[0m \u001b[39melif\u001b[39;00m startswith(\u001b[39m\"\u001b[39m\u001b[39m<!--\u001b[39m\u001b[39m\"\u001b[39m, i):\n\u001b[0;32m    174\u001b[0m     k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_comment(i)\n",
      "File \u001b[1;32mc:\\Users\\Roger\\miniconda3\\envs\\data_analytics\\lib\\html\\parser.py:420\u001b[0m, in \u001b[0;36mHTMLParser.parse_endtag\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m    417\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_data(rawdata[i:gtpos])\n\u001b[0;32m    418\u001b[0m         \u001b[39mreturn\u001b[39;00m gtpos\n\u001b[1;32m--> 420\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle_endtag(elem)\n\u001b[0;32m    421\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclear_cdata_mode()\n\u001b[0;32m    422\u001b[0m \u001b[39mreturn\u001b[39;00m gtpos\n",
      "File \u001b[1;32mc:\\Users\\Roger\\miniconda3\\envs\\data_analytics\\lib\\site-packages\\bs4\\builder\\_htmlparser.py:193\u001b[0m, in \u001b[0;36mBeautifulSoupHTMLParser.handle_endtag\u001b[1;34m(self, name, check_already_closed)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malready_closed_empty_element\u001b[39m.\u001b[39mremove(name)\n\u001b[0;32m    192\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 193\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msoup\u001b[39m.\u001b[39;49mhandle_endtag(name)\n",
      "File \u001b[1;32mc:\\Users\\Roger\\miniconda3\\envs\\data_analytics\\lib\\site-packages\\bs4\\__init__.py:742\u001b[0m, in \u001b[0;36mBeautifulSoup.handle_endtag\u001b[1;34m(self, name, nsprefix)\u001b[0m\n\u001b[0;32m    736\u001b[0m \u001b[39m\"\"\"Called by the tree builder when an ending tag is encountered.\u001b[39;00m\n\u001b[0;32m    737\u001b[0m \n\u001b[0;32m    738\u001b[0m \u001b[39m:param name: Name of the tag.\u001b[39;00m\n\u001b[0;32m    739\u001b[0m \u001b[39m:param nsprefix: Namespace prefix for the tag.\u001b[39;00m\n\u001b[0;32m    740\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    741\u001b[0m \u001b[39m#print(\"End tag: \" + name)\u001b[39;00m\n\u001b[1;32m--> 742\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendData()\n\u001b[0;32m    743\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popToTag(name, nsprefix)\n",
      "File \u001b[1;32mc:\\Users\\Roger\\miniconda3\\envs\\data_analytics\\lib\\site-packages\\bs4\\__init__.py:588\u001b[0m, in \u001b[0;36mBeautifulSoup.endData\u001b[1;34m(self, containerClass)\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_only \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtagStack) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    584\u001b[0m        (\u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_only\u001b[39m.\u001b[39mtext \u001b[39mor\u001b[39;00m \\\n\u001b[0;32m    585\u001b[0m         \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_only\u001b[39m.\u001b[39msearch(current_data)):\n\u001b[0;32m    586\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> 588\u001b[0m containerClass \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstring_container(containerClass)\n\u001b[0;32m    589\u001b[0m o \u001b[39m=\u001b[39m containerClass(current_data)\n\u001b[0;32m    590\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobject_was_parsed(o)\n",
      "File \u001b[1;32mc:\\Users\\Roger\\miniconda3\\envs\\data_analytics\\lib\\site-packages\\bs4\\__init__.py:506\u001b[0m, in \u001b[0;36mBeautifulSoup.string_container\u001b[1;34m(self, base_class)\u001b[0m\n\u001b[0;32m    503\u001b[0m \u001b[39m# On top of that, we may be inside a tag that needs a special\u001b[39;00m\n\u001b[0;32m    504\u001b[0m \u001b[39m# container class.\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstring_container_stack \u001b[39mand\u001b[39;00m container \u001b[39mis\u001b[39;00m NavigableString:\n\u001b[1;32m--> 506\u001b[0m     container \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuilder\u001b[39m.\u001b[39;49mstring_containers\u001b[39m.\u001b[39;49mget(\n\u001b[0;32m    507\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstring_container_stack[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mname, container\n\u001b[0;32m    508\u001b[0m     )\n\u001b[0;32m    509\u001b[0m \u001b[39mreturn\u001b[39;00m container\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "r = session.get('https://reddit.com')\n",
    "for html in r.html:\n",
    "    print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_oi_bku2ItU"
   },
   "source": [
    "Podemos observar que el html que devuelve es solo 1. Esto es porque el contenido se va actualizando y añadiendo dinámicamente mediante \"scrolldown\" y no mediante el click de un boton de Pagina siguiente, Pagina 2, o similares. En caso de haber un segundo html, este hace referencia al boton de introducir una cuenta reddit. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6kZWiN_p83uC"
   },
   "source": [
    "Ahora, después de inspeccionar la página, advertimos que el contenido relativo a los enlaces de las diferentes entradas o noticias de la página, está en la etiqueta 'a' y cuya classe es '_3ryJoIoycVkA88fy40qNJc'. Por tanto, usando selectores CSS, la instrucción que devolverá el contenidos será: *r.html.find('a._3ryJoIoycVkA88fy40qNJc')*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "YLYQts748d_-",
    "outputId": "38c57b0d-bccd-4a81-f051-29f65dbd4c47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 titulares aparecen en la primera página de reddit, el resto de titulares se corresponden con el contenido después de hacer scrolldowns\n"
     ]
    }
   ],
   "source": [
    "subreddit_1 = r.html.find('a._3ryJoIoycVkA88fy40qNJc')\n",
    "print(str(len(subreddit_1)) + ' titulares aparecen en la primera página de reddit, el resto de titulares se corresponden con el contenido después de hacer scrolldowns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "id": "jO9Nthx2-gTK",
    "outputId": "292c4693-3d31-421d-eebd-61d25ff3f96f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'https://www.reddit.com/r/spain/'},\n",
       " {'https://www.reddit.com/r/mildlyinfuriating/'},\n",
       " {'https://www.reddit.com/r/AskReddit/'},\n",
       " {'https://www.reddit.com/r/interestingasfuck/'},\n",
       " {'https://www.reddit.com/r/Genshin_Impact_Leaks/'},\n",
       " {'https://www.reddit.com/r/LMDShow/'},\n",
       " {'https://www.reddit.com/r/Genshin_Impact_Leaks/'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtener las url completas de cada entrada\n",
    "subreddit_url=[element.absolute_links for element in subreddit_1]\n",
    "subreddit_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bsoi9zZS-xev"
   },
   "source": [
    "Ahora vamos a obtener los títulos de las noticias. Este contenido se halla en la etiqueta 'h3' con classe '_eYtD2XCVieq6emjKBH3m'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "id": "ZGfKcOZ_-s4Y",
    "outputId": "999c2618-edf5-402a-8f41-246fee0f24ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['🤔',\n",
       " 'Um gajo pode sonhar não pode?',\n",
       " 'Los Santos Drug Wars inyectará un nuevo caos psicoactivo en GTA Online el 13 de diciembre. Únete a una banda de inadaptados en el primer capítulo de una extensa nueva actualización de GTA Online, con una trama dividida en dos partes.',\n",
       " 'The fees on this Air BnB for one night.',\n",
       " 'What did you not know about sex until you lost your virginity?',\n",
       " \"U.S. bombs dropped on Laos. 270 million bombs were dropped on Laos in a span of 9 years, making it the most heavily bombed country in the history of the world. That's 57 bombs every minute on average.\",\n",
       " 'Alhaitham gameplay (R.I.P)',\n",
       " 'escucha escucha ☝️🤓',\n",
       " 'Yaoyao Gameplay']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Titulos\n",
    "subreddit_2 = r.html.find('h3._eYtD2XCVieq6emjKBH3m')\n",
    "subreddit_titulos=[element.text for element in subreddit_2]\n",
    "subreddit_titulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "pnyQxO56_8qf",
    "outputId": "9bcd54c8-6847-47b4-a0e8-b0912c2e9ff4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['357', '•', '17.0k', '10.9k', '20.8k', '4.7k', '412', '2.1k']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Votaciones\n",
    "subreddit_3 = r.html.find('div._1rZYMD_4xY3gRcSS3p8ODO._25IkBM0rRUqWX5ZojEMAFQ')\n",
    "subreddit_votaciones=[element.text for element in subreddit_3]\n",
    "subreddit_votaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7VLcfGBHcHh"
   },
   "source": [
    "Por último, cerraremos la sesión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "EPYBbWQiHmAa"
   },
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7H66P1kAjHT"
   },
   "source": [
    "Ahora vamos a construir el dataframe que contiene el resultado de los títulos y las voraciones. Atendiendo a los datos, el primer titular sin puntuación lo vamos a excluir porque hace referencia a un anuncio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "4QGsFYZbBboW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_reddit=pd.DataFrame()\n",
    "df_reddit['título']=subreddit_titulos[1:]\n",
    "df_reddit['votaciones']=subreddit_votaciones[1:]\n",
    "df_reddit['url']=subreddit_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>título</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Um gajo pode sonhar não pode?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Los Santos Drug Wars inyectará un nuevo caos psicoactivo en GTA Online el 13 de diciembre. Únete a una banda de inadaptados en el primer capítulo de una extensa nueva actualización de GTA Online, con una trama dividida en dos partes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The fees on this Air BnB for one night.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What did you not know about sex until you lost your virginity?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U.S. bombs dropped on Laos. 270 million bombs were dropped on Laos in a span of 9 years, making it the most heavily bombed country in the history of the world. That's 57 bombs every minute on average.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Alhaitham gameplay (R.I.P)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>escucha escucha ☝️🤓</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Yaoyao Gameplay</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                      título\n",
       "0  Um gajo pode sonhar não pode?                                                                                                                                                                                                            \n",
       "1  Los Santos Drug Wars inyectará un nuevo caos psicoactivo en GTA Online el 13 de diciembre. Únete a una banda de inadaptados en el primer capítulo de una extensa nueva actualización de GTA Online, con una trama dividida en dos partes.\n",
       "2  The fees on this Air BnB for one night.                                                                                                                                                                                                  \n",
       "3  What did you not know about sex until you lost your virginity?                                                                                                                                                                           \n",
       "4  U.S. bombs dropped on Laos. 270 million bombs were dropped on Laos in a span of 9 years, making it the most heavily bombed country in the history of the world. That's 57 bombs every minute on average.                                 \n",
       "5  Alhaitham gameplay (R.I.P)                                                                                                                                                                                                               \n",
       "6  escucha escucha ☝️🤓                                                                                                                                                                                                                      \n",
       "7  Yaoyao Gameplay                                                                                                                                                                                                                          "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "DaWKTgJiB02L",
    "outputId": "9cd16d1f-f791-4c79-f022-553cf6f30333"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Roger\\AppData\\Local\\Temp\\ipykernel_2300\\2182009870.py:1: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>título</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Um gajo pode sonhar não pode?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Los Santos Drug Wars inyectará un nuevo caos psicoactivo en GTA Online el 13 de diciembre. Únete a una banda de inadaptados en el primer capítulo de una extensa nueva actualización de GTA Online, con una trama dividida en dos partes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The fees on this Air BnB for one night.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What did you not know about sex until you lost your virginity?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U.S. bombs dropped on Laos. 270 million bombs were dropped on Laos in a span of 9 years, making it the most heavily bombed country in the history of the world. That's 57 bombs every minute on average.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Alhaitham gameplay (R.I.P)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>escucha escucha ☝️🤓</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Yaoyao Gameplay</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                      título\n",
       "0  Um gajo pode sonhar não pode?                                                                                                                                                                                                            \n",
       "1  Los Santos Drug Wars inyectará un nuevo caos psicoactivo en GTA Online el 13 de diciembre. Únete a una banda de inadaptados en el primer capítulo de una extensa nueva actualización de GTA Online, con una trama dividida en dos partes.\n",
       "2  The fees on this Air BnB for one night.                                                                                                                                                                                                  \n",
       "3  What did you not know about sex until you lost your virginity?                                                                                                                                                                           \n",
       "4  U.S. bombs dropped on Laos. 270 million bombs were dropped on Laos in a span of 9 years, making it the most heavily bombed country in the history of the world. That's 57 bombs every minute on average.                                 \n",
       "5  Alhaitham gameplay (R.I.P)                                                                                                                                                                                                               \n",
       "6  escucha escucha ☝️🤓                                                                                                                                                                                                                      \n",
       "7  Yaoyao Gameplay                                                                                                                                                                                                                          "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "df_reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tD5CuNtYxtPb"
   },
   "source": [
    "En este ejemplo, hemos podido scrapear el contenido inicial que aparece en la página de Reddit. No obstante, el contenido que encontramos al hacer _scrolldown_ no lo hemos podido capturar.  \n",
    "\n",
    "Para poder capturar el resto de entradas, podemos hacer uso de la funcion *render()* de la librería *Request-html*. No obstante, para el uso de esta función es necesario iniciar una sesión en modo asincrona. Los Jupyter notebooks presentan ciertos problemas para trabajar de esta forma y requieren la instalación de _Chromium_; es por eso que en esta PEC no lo vamos a ver. A pesar de ello, a modo explicativo, se adjunta el código que se requeriría:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "import asyncio\n",
    "from requests_html import AsyncHTMLSession\n",
    "\n",
    "asession = AsyncHTMLSession()\n",
    "\n",
    "async def get_results():\n",
    "    r = await asession.get('https://reddit.com')\n",
    "    r.html.arender(scrolldown=10, sleep=1)\n",
    "    return r\n",
    "\n",
    "respuesta = asession.run(get_results)\n",
    "\n",
    "asession.close()\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p95Elq6CIWDQ"
   },
   "source": [
    "Otro ejemplo es la obtención de información de una página que recoge los memes más relevantes en la actualidad (www.knowyourmeme.com). En esta página, a diferencia del ejemplo anterior, el contenido está organizado por páginas y se puede acceder a ellas a través de los típicos botones de páagina 1, 2, 3,... Vamos a ver, por tanto, la respuesta que nos devolvería requests-html en este caso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "3KRonTtQKoHk"
   },
   "outputs": [],
   "source": [
    "# Cargar librería\n",
    "from requests_html import HTMLSession\n",
    "\n",
    "# Iniciar sesión\n",
    "session = HTMLSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "UwVnuDSNLOX1",
    "outputId": "a16a1e0d-796f-4d8b-a3dd-48558e4c85aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 :  <HTML url='https://knowyourmeme.com/'>\n",
      "2 :  <HTML url='https://knowyourmeme.com/page/2'>\n",
      "3 :  <HTML url='https://knowyourmeme.com/page/3'>\n",
      "4 :  <HTML url='https://knowyourmeme.com/page/4'>\n",
      "5 :  <HTML url='https://knowyourmeme.com/page/5'>\n",
      "6 :  <HTML url='https://knowyourmeme.com/page/6'>\n",
      "7 :  <HTML url='https://knowyourmeme.com/page/7'>\n",
      "8 :  <HTML url='https://knowyourmeme.com/page/8'>\n",
      "9 :  <HTML url='https://knowyourmeme.com/page/9'>\n",
      "10 :  <HTML url='https://knowyourmeme.com/page/10'>\n"
     ]
    }
   ],
   "source": [
    "r2= session.get('https://knowyourmeme.com/')\n",
    "\n",
    "total_pags_scrap=10\n",
    "page=0\n",
    "for html in r2.html:\n",
    "    page+=1\n",
    "    print(page, ': ' ,html)\n",
    "    if page==total_pags_scrap:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "UxHydcKDMkKn"
   },
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1LJBx70NqQ4"
   },
   "source": [
    "En este caso, la respuesta contine los html asociados a las diferentes páginas en las que está organizado el contenido. Como podéis observar, en el bucle se ha introducido un _break_. Esto es porque knowyourmeme.com ofrece la posibilidad de revisar los memes contenidos en  hasta más de 9500 páginas. Esperar a scrapear este total de páginas nos llevaría mucho más tiempo y no es el objetivo de esta PEC.\n",
    "\n",
    "Considerando los ejemplos que se han facilitado, realizar el ejercicio práctico 3 para explorar la librería Requests-html y familiarizarse con los selectores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9r0z0MdxbSs4"
   },
   "source": [
    "### **Ejercicio práctico 2 (vuelos real time)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTucSQJybd9F"
   },
   "source": [
    "La API OpenSky (https://opensky-network.org/apidoc/rest.html) permite recuperar información del estado de los vuelos o aeronaves actualizados en tiempo real. Además, los usuarios con autorización pueden tener acceso a datos históricos de la última hora, indicando el intervalo de tiempo cómo un parámetro más en la consulta. Para esta actividad vamos a utilizar la versión básica de esta API y por tanto, no vamos a tener opción de seleccionar el intervalo de tiempo de interés. No obstante, para la finalidad de la actividad, no va a hacer falta. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SwHLlsi7bjiW"
   },
   "source": [
    "En este ejercicio se solicita al estudiante que, después de la revisión de la documentación de la API, obtenga el estado del aire (aviones y aeronaves) durante una secuencia de 10 consultas. Para ello, se recomienda que se implente un bucle donde en cada iteración se solicite información a la API y se esperen 10 segundos entre una iteración y la siguiente. La solicitud a la API se pide que se realice mediante la librería requests, con la que se trabajó en la PEC anterior. \n",
    "\n",
    "Mencionar que esta API permite añadir en la solicitud una acotación del espacio a considerar. La acotación del espacio aéreo se corresponderá con los siguientes valores de latitud i longitud:\n",
    "\n",
    "> lat_max= 72.822950; \n",
    "lon_min= -17.000158;\n",
    "lat_min= 35.550423;\n",
    "lon_max= 44.022436\n",
    "\n",
    "Para cada consulta, se solicita obtener el número total de aeronaves agrupadas por país de origen de la aeronave. Para ello, será necesario revisar la documentación de la API y explorar la forma de la respuesta para determinar cuál es el campo de interés de los datos scrapeados para la realización del ejercicio. \n",
    "Además del número de aeronaves por país de origen, en cada iteración también hay que almacenar el valor temporal de la consulta (es decir, el campo 'time', que viene dado en segundos ).\n",
    "\n",
    "Con el resultado de las 10 iteraciones, se solicita que:\n",
    "- Se cree un dataframe (df_vuelos) que recoja el número total de aeronaves por cada país de origen (filas) para cada instante de tiempo (columnas). \n",
    "- Se añada una columna 'mean_flights' y una 'percen_flights' que representen respectivamente el valor medio de vuelos de cada país durante el intervalo de tiempo considerado con las 10 iteraciones y el porcentaje del país respecto al total.\n",
    "- Representar con un diagrama de barras el número de vuelos medios de los 15 países con mayor media de vuelos en el intervalo de tiempo considerado. El eje X, se corresponderá con el país y el eje Y con el valor de vuelos medios. Mostrar los valores ordenados de forma ascendiente. Mostrar también el porcentaje agregado de estos 15 países respecto al total.\n",
    "- Repetir el putno anterior para los 15 países con menor media."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deZfeuJ6n67S"
   },
   "source": [
    "NOTAS PARA LA REALIZACIÓN DE LAS SOLICITUDES Y LA CREACIÓN DEL DATAFRAME: \n",
    "- Para considerar el tiempo de espera, como ya hemos hecho en casos anteriores, se recomienda el uso de la librería time y su función time.sleep(n_segundos).\n",
    "- Debido a que estamos usando la versión gratuita de la API, el tiempo no se actualiza de forma constante y puede que haya solicitudes cuyo valor de 'time' es el mismo. En este caso, no almacenar el valor o eliminar después los valores cuyo 'time' esté duplicado. Esto es para que cuando calculemos la media, no tenga efecto en la misma el valor duplicado.\n",
    "- _Sugerencia:_ En cada iteración se puede crear un dataframe auxiliar con los resultados de la solicitud y cuyas columnas sean 'Country' y 'N' para el 'time' evaluado en dicha iteración. Después, con este dataframe auxiliar, ir hacido join o merge al dataframe glogal que contendrá los resultados de todas las iteraciones. Si se sigue esta sugerencia, utilizar la versión 'outer' de la función merge de fomra que se consideren todos los países que han aparecido en todas las iteraciones aunque en una determinada iteración no hubiese aeronaves de alguno de ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opensky_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "qEEfCaAvb69-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 :  <HTML url='https://opensky-network.org/apidoc/rest.html'>\n"
     ]
    }
   ],
   "source": [
    "#Cargar librerias\n",
    "import requests\n",
    "\n",
    "session = HTMLSession()\n",
    "\n",
    "r2= session.get('https://opensky-network.org/apidoc/rest.html')\n",
    "\n",
    "total_pags_scrap=10\n",
    "page=0\n",
    "for html in r2.html:\n",
    "    page+=1\n",
    "    print(page, ': ' ,html)\n",
    "    if page==total_pags_scrap:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir limites Europa\n",
    "#TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hs1Hq70Tb_DU"
   },
   "outputs": [],
   "source": [
    "# Creación de df_vuelos mediante la realización de 10 solicitudes\n",
    "#TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostrar df_vuelos\n",
    "#TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear nuevas variables: 'mean_flights' y 'percen_flights'\n",
    "#TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grafico de barras (15 países con más vuelos) + calcular porcentaje de esos 15 primeros paises\n",
    "#TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yZgLoHcjcBWj"
   },
   "outputs": [],
   "source": [
    "# Grafico de barras (15 países con menos vuelos)\n",
    "#TODO\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ZYIvtVsuFTrD"
   ],
   "name": "CyPD_PEC3_2020_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "data_analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "b824bf52cf534a16238dce884e8fdba8cd1853727e048048d6b0b37b0d718f27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
