{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ck4PufGcoNb2"
   },
   "source": [
    "# PEC 3 - Web Scraping Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KC-F1owwXIEv"
   },
   "source": [
    "En esta PEC vamos a **continuar trabajando el web scraping**. Vamos a prestar especial atención al web scraping en streaming que es el objetivo del reto. Además, continuaremos explorando otras librerías que nos permiten hacer web scraping, como request-html y SerPapi.\n",
    "\n",
    "Por tanto, la PEC se va a dividir en **3 PARTES**: Web Scraping en Streaming, Web Scraping con Requests-html y, Web Scraping con SerPapi.\n",
    "\n",
    "Mencionar que, en algunos ejercicios se va a motivar el uso de los selectores CSS y los XPath. \n",
    "**Los selectores CSS y XPath** son expresiones que permiten seleccionar elementos de un documento HTML basados en sus clases o en la ubicación dentro del contenido. \n",
    "Una referencia interesante de los mismos la podéis encontrar en las siguientes dos páginas web: https://www.w3schools.com/xml/xpath_syntax.asp, https://www.w3schools.com/cssref/css_selectors.asp "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXE7YpuEXzMf"
   },
   "source": [
    "_Ejemplo:_\n",
    "\n",
    "\n",
    "\n",
    "*   _p.intro.rellevant_: seleccionaría los elementos _párrafo_ con valores de classe iguales a 'intro' y 'rellevant'.  \n",
    "*   _div > p_ : selecciona todos los elementos _\\<p>_ donde el padre sea un elemento \\<div>.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DyPw4Ratkvr"
   },
   "source": [
    "### **Ejercicio práctico 3 (Nature)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ieI9ZagPoIs"
   },
   "source": [
    "NATURE (https://www.nature.com/) es una de las más prestigiosas revistas científicas a nivel mundial. En su página web podemos encontrar distinta información como artículos de opinión, noticias, vídeos, libros y también artículos de investigación científica. Si dentro de la pestaña _Explore content_ nos vamos a _Research articles_ podemos encontrar todos los artículos científicos que se van publicando, con opción de filtrar por tema y año, y que pueden ser consultadas a través del link que tiene el mismo título. Mencionar que solamente se puede leer el _abstract_ de los artículos ya que el acceso de los artículos enteros es un servicio de pago.\n",
    "\n",
    "En este ejercicio vamos a hacer scraping de los artículos científicos publicados más recientemente, sin importar la temática. Esto se corresponderá en hacer scraping de la siguiente dirección web: 'https://www.nature.com/nature/research-articles'\n",
    "\n",
    "**Parte 1.** Se solicita:\n",
    "\n",
    "- Obtener mediante scraping y haciendo uso de selectores la siguiente información de cada publicación o referencia que aparece en el resultado de la búsqueda (es decir, en la página web 'https://www.nature.com/nature/research-articles'):\n",
    "\n",
    "  - Título de la publicación\n",
    "  - Autores de la publicación\n",
    "  - URL completa de la entrada en NATURE para cada publicación\n",
    "  - Fecha de publicación\n",
    "\n",
    "- Organizar la información scrapeada en forma de dataframe (df_nature_1), de tal forma que cada punto objeto de scraping se corresponda con una columna. Es decir, las columnas serán: 'titulo', 'autores', 'url', 'fecha'.\n",
    "\n",
    "Haciendo scraping de la dirección web facilitada, hemos obtenido información de los resultados que se facilitan en la primera página de resultados. No obstante, si exploramos Nature podemos ver que hay muchas más páginas con resultados de la búsqueda realizada. \n",
    "\n",
    "**Parte2.** Se solicita: \n",
    "- Obtener un dataframe (df_pnature_2) como el obtenido del punto anterior, pero que contenga la información de las siguientes 4 páginas de resultados (páginas 2 a 5). Para ello, se debe explorar y observar cómo cambia la url objeto de scraping cuando vamos pasando de páginas. Tras haber observado la morfología de la url, plantear un bucle (similar al proceso realizado en el ejercicio 3) que consiga realizar el el proceso de scraping planteado en el primer punto de este ejercicio para las 4 siguientes páginas de resultados. \n",
    "\n",
    "\n",
    "**Parte3.** En la entrada de cada publicación podemos, entre otra información, consultar el abstract. Por tanto, se solicita en este punto:\n",
    "\n",
    "- obtener, un diccionario que contenga la url y el abstract de las publicaciones que forman parte del df_nature_1. Las claves del diccionario serán los títulos de las publicaciones y los valores para cada clave será un diccionario con dos elementos correspondientes a la url y el abstract.\n",
    "\n",
    "La forma del diccionario solicitado es la siguiente:\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "{'Titulo 1': {'url': valor de la url\n",
    "              'abstract': contenido del abstract\n",
    "             }, \n",
    "\n",
    "'Titulo 2': {...\n",
    "\n",
    "            }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WqnwzYmTtv5a"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests_html import HTMLSession\n",
    "from bs4 import BeautifulSoup as bs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JpVXfp4dWQi6"
   },
   "source": [
    "**PARTE 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.nature.com/articles/s41586-022-05621-0',\n",
       " 'https://www.nature.com/articles/s41586-022-05607-y',\n",
       " 'https://www.nature.com/articles/s41586-022-05477-4',\n",
       " 'https://www.nature.com/articles/s41586-022-05404-7',\n",
       " 'https://www.nature.com/articles/s41586-022-05390-w',\n",
       " 'https://www.nature.com/articles/s41586-022-05510-6',\n",
       " 'https://www.nature.com/articles/s41586-022-05348-y',\n",
       " 'https://www.nature.com/articles/s41586-022-05499-y',\n",
       " 'https://www.nature.com/articles/s41586-022-05327-3',\n",
       " 'https://www.nature.com/articles/s41586-022-05513-3',\n",
       " 'https://www.nature.com/articles/s41586-022-05466-7',\n",
       " 'https://www.nature.com/articles/s41586-022-05501-7',\n",
       " 'https://www.nature.com/articles/s41586-022-05504-4',\n",
       " 'https://www.nature.com/articles/s41586-022-05509-z',\n",
       " 'https://www.nature.com/articles/s41586-022-05511-5',\n",
       " 'https://www.nature.com/articles/s41586-022-05495-2',\n",
       " 'https://www.nature.com/articles/s41586-022-05450-1',\n",
       " 'https://www.nature.com/articles/s41586-022-05507-1',\n",
       " 'https://www.nature.com/articles/s41586-022-05502-6',\n",
       " 'https://www.nature.com/articles/s41586-022-05403-8']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_original = 'https://www.nature.com'\n",
    "r = requests.get(url_original + '/nature/research-articles')\n",
    "\n",
    "soup_nature = bs(r.text, 'lxml')\n",
    "nature_grid = soup_nature.find_all(class_='c-card__link u-link-inherit')\n",
    "articles = [url_original + link['href'] for link in nature_grid]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JpVXfp4dWQi6"
   },
   "source": [
    "**PARTE 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = HTMLSession()\n",
    "\n",
    "r= session.get('https://www.nature.com/nature/research-articles')\n",
    "\n",
    "for html in r.html:\n",
    "    print(html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear dataframe con resultados\n",
    "#TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JpVXfp4dWQi6"
   },
   "source": [
    "**PARTE 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos mostramos el diccionario con la URL y el abstract\n",
    "#TODO\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ZYIvtVsuFTrD"
   ],
   "name": "CyPD_PEC3_2020_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "data_analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "b824bf52cf534a16238dce884e8fdba8cd1853727e048048d6b0b37b0d718f27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
